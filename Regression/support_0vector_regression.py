# -*- coding: utf-8 -*-
"""Support_0vector_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wPMQWppNGmrOO39Jc61ryTTettrJ3r8

# Support Vector Regression (SVR)
In simple linear regression , curve is plotted to minimise the difference b/w actual to that of predicted value using Least square method (Sum(y1-y1ˆ)ˆ2)

SVR : Regression method in which curve is plotted using epsilon tube on either sides with distance = epsilon from regression line. Data points inside our tube will be disregarding of error (no error if points inside tube) , means tube is a margin of error that we're allowing & not caring about error inside here.

Points outside our Epsilon tube are to be considered for error & measured by distance b/w tube & datapoint lying outside.

Ei+(Eiˆ*) - point above tube + below to be minimised

Called as support vector regression as every points represent a vector & points lying outside are **SUPPORT VECTORS** as they dictating the tube structure & how it's created

Any point outside the Epsilon tube is a Support Vector.

Support vector Regression is a Non linear regression Model.

Feature scaling is done priorly in SVR as in linear regression we have coefficients that can deal with high values of features which is missing in SVR.

To implement relation b/w Independent var X and dependent var y , feature scaling is required.

Not required to split dataset in Train & Test data in SVR as maximum data/leverage data required for training to implement a correlation b/w position level and salary.

In datapreprocessing part ,feature scaling is applied only to X_test & train not y ,because we have y matrix as 0 & 1 values only.

We need to apply feature scaling to matrix y also as we have Salary as varying values & we don't have any equation like in linear & multiple regression (y=b0+b1x1+--) here.

SVR Model will not work at all , if we don't apply feature scaling as model will neglect the varying values.

Also Feature scaling is not applied to DUMMY Variable obtained via onehotencoder and dependent var with binary values(0&1) as values are already in the right range.

Inverse feature scaling is also done to obtain back transformed values.

Mean and standard deviation is applied on columns in feature scaling.

Two different Standard scaler object to be created so as to perform mean & Standard deviation on both matrices.

Standarisation scale values b/w +3 & -3

Sklearn - for machine learning
Tensorflow & Pytorch - library for Deep learning (neural networks)

## Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## Importing the dataset"""

dataset = pd.read_csv('Position_Salaries.csv')
X=dataset.iloc[:,1:-1].values
y=dataset.iloc[:,-1].values

print(X)
print("\n")
print(y)

"""# Reshape y into 2d array where salarys displayed vertically

Required as for feature scaling using StandardScaler , input required is 2d array vertically.

1d Horizontal array will throw an error.
"""

# a.reshape(shape,order='C')
# a.reshape(no of rows,columns)
# no of rows = len(y)

y=y.reshape(len(y),1)
print(y)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc_x =StandardScaler()
sc_y = StandardScaler()
X=sc_x.fit_transform(X)
y=sc_y.fit_transform(y)

print(X)      # scaled b/w -1.5 to 1.5
print("\n")
print(y)      # scaled b/w -0.7 to 2.6

"""## Training the SVR model on the whole dataset"""

#Using svm module of sklearn library to import SVR class to implement support vector regression.
# Regressor object/instance of SVR class to be created
# Kernel fxns as linear , polynomial gaussian, radial ,sigmoid etc

from sklearn.svm import SVR
regressor = SVR(kernel='rbf')   #rbf = radial basis fxn
regressor.fit(X,y)

"""# Predicting a new result"""

#We're required to Reverse the scaling of your prediction & reshape y to original matrix in order to predict the
#new result
#inverse transform y and X to predict value
#Reshape need to be done to avoid format error as previously reshaped y may cause error while predicting.

y_pred = sc_y.inverse_transform(regressor.predict(sc_x.transform([[6.5]])).reshape(-1,1))
print(y_pred)

"""## Visualising the SVR results"""

# To plot b/w original values , we need to inverse transform of martrix X & y.
# Scatter plots for original values
# Plot plots for predicted values

plt.scatter(sc_x.inverse_transform(X),sc_y.inverse_transform(y),color='orange')
plt.plot(sc_x.inverse_transform(X),sc_y.inverse_transform(regressor.predict(X).reshape(-1,1)),color='red')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position levels')
plt.ylabel('Salary')
plt.show()

#Accurate outcomes with outliers (last one (1,10))

"""## Visualising the SVR results (for higher resolution and smoother curve)"""

X_grid = np.arange(min(sc_x.inverse_transform(X)), max(sc_x.inverse_transform(X)), 0.1)     #More no of datapoints as gap is 0.1, with arranged in ascending order - density increased - (1 ,1.1 ,1.2---9.9,10)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(sc_x.inverse_transform(X),sc_y.inverse_transform(y), color = 'red')
plt.plot(X_grid , sc_y.inverse_transform(regressor.predict(sc_x.transform(X_grid)).reshape(-1,1)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

#Much more accuracy & resolution obtained