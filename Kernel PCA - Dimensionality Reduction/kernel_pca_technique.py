# -*- coding: utf-8 -*-
"""Kernel_PCA_Technique.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caIDL0Orza6lT1Z7I546BC81VTBbPZXW

# Kernel PCA
"""

#Kernel trick : Mapping the data into higher Dimension and then separating classes

#Kernel PCA is an extension of PCA that allows for nonlinear dimensionality reduction. It applies the kernel trick,
#commonly used in Support Vector Machines (SVMs), to transform the data into a higher-dimensional feature space where linear
#separation of data becomes possible.

#Done on Unsupervised data

#Nonlinear dimensionality reduction using kernel trick.
#Uses kernel trick to implicitly compute higher-dimensional feature space.
#Handles nonlinear data relationships (e.g., image recognition).

#When the dataset is not linearly separable, and dimensionality reduction is applied to it using PCA technique, then Kernel trick
#is applied before PCA on dataset to map data into higher dimension, find correlation & then classify.

#As one wrong prediction observed in PCA was due to that datapoint was not linearly separable, hence after mapped to high dimension,
#it will also be predicted well in it's class itself using Kernel Trick. (Imagine , for reference SVC Slides )


# Kernel PCA is indeed used to reduce dimensionality, especially when dealing with high-dimensional data.
# It allows for non-linear relationships to be captured, which can improve classification tasks by reducing the complexity of the data representation.
# Kernel PCA does map the data into a higher-dimensional space using a kernel function to make it linearly separable or capture non-linear relationships.
# However, PCA itself is applied after this mapping to reduce the dimensionality in the new feature space, not to make it linearly separable.
# PCA in Kernel PCA is used to find the principal components in the transformed space, not to ensure linear separability.

"""## Importing the libraries"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Wine.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Applying Kernel PCA"""

#For Kernel Trick + PCA , decompostion module contain KernelPCA class for it's implementation
#Kernel = radial based function rbf used

from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components = 2, kernel = 'rbf') #Then creating object of class to implement the function
X_train = kpca.fit_transform(X_train)
X_test = kpca.transform(X_test)

"""## Training the Logistic Regression model on the Training set"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#As expected 100% accuracy with Kernel PCA on Logistic Regression model is achieved.
#KernelPCA beats PCA & obtained that wrong prediction correct.

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

#Graph obtained in PCA was in 2dimension , but kernel trick mapped to 3dimension here , see visuals.
#Try on other multiple featured dataset (more than 10 features/independent var) from UCI ML dataset repository for strong grasp.

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()