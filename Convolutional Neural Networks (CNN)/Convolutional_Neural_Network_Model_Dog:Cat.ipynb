{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "Classification of I/P image as Dog or Cat based on the trained data in CNN Using Deep Learning algortihm.\n",
        "\n",
        "\n",
        "\n",
        "The Implementation of CNN is done on Jupyter Notebook as due to large number of dataset & size = megabytes of data with 8,000 sample images.\n",
        "\n",
        "\n",
        "To run and view the implementation, we need to use the Jupyter Notebook in CNN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JInHO7QvNg_-",
        "outputId": "3aa816de-5314-4db8-b5ce-4eb77a894019"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.16.2'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "tf.__version__\n",
        "\n",
        "#ImageDataGenarator class of image submodule of preprocessing module from keras library is used to preprocess the dataset\n",
        "\n",
        "#Only tensorflow and keras library required to implement deep learning CNN algo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "qcK6O4luQpxZ",
        "outputId": "5042f772-3fca-4640-e6cf-bc752c533e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#Data preprocessing in this section is not on the Tabular integral value dataset,\n",
        "#But preprocessing of the dataset having Images within it for classification using CNN\n",
        "\n",
        "#TRY : We can also combine NLP+Deep Learning Through Keras lib to build a text model, give i/p as text & o/p as response from CNN\n",
        "\n",
        "#We're going to implement transformation on our image dataset , so as to avoid Overfitting - 98% accuracy on Train set, 43% on test set\n",
        "#If we won't apply transformation on the image dataset, there will be huge difference in accuracy of training to test predictions.\n",
        "#Transformations are geometrical shifts - rotate , zoom , stretch , flips and series of transformation\n",
        "#Also called Augmentation of dataset - augment the variety of image train-set , so as model won't overlearn  / overtrain.\n",
        "\n",
        "#train_datagen is object of ImageDataGenerator class of Keras Library\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,       #Importing all Transform features to apply & preserving in train_datagen object/instance, ImageDatagenerator class of Keras will augment/transform the image\n",
        "                                   shear_range = 0.2,      #Feature scaling is Utterly IMP in Neural network , each pixel of image is in range 0-255 , so scalling b/w 0 & 1 by dividing each pixel by 255\n",
        "                                   zoom_range = 0.2,       #Applying Shear, zoom and horizontal flip\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "#Now importing the dataset & applying these transformation features to it and creating batches\n",
        "training_set = train_datagen.flow_from_directory(   #flow_from_directory is the method of ImageDataGenerator to connect the transformation features to training data\n",
        "        'dataset/training_set',                     #Directory/location/path of our training data\n",
        "        target_size = (64, 64),                     #Final size of the Images fed to Convolutional neural network = 64 by 64\n",
        "        batch_size = 32,                            #Creating batches of size 32 within train data\n",
        "        class_mode = 'binary'                       #O/p is binary or value , as classification have binary o/p = Cat/Dog\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdUJaSatYNP8",
        "outputId": "eee320a8-df2c-4749-ff0f-cff7e116071e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#We're not gonna apply transformation features to test set , just apply the feature scaling on the test set, keep them original\n",
        "#Just like fit_transform on train set & only transform on test set\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH0K-LTMtv_e"
      },
      "outputs": [],
      "source": [
        "#Before feeding the I/p to ANN , we've to create 3 layer - Convolution  MaxPooling and flattening layer to perform their tasks,\n",
        "#After that Flattened layer is connected to Ann i/p layer for classification\n",
        "\n",
        "#Implementation of CNN is same as the ANN Just the before convolution is applied to the dataset\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "#cnn is created as object/instance of Sequential class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iorVY-FquMmf",
        "outputId": "36580834-77f6-4798-d644-473bf53aefae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Applying the convolution on images matrix to create the convolved feature map\n",
        "#ConV2D IS the function of layers class to perform convolution on our image dataset\n",
        "#Parameters given to ConV2D are no of filters/feature detectors to be applied to image,\n",
        "#kernel_size = size of feature detector = 3 by 3 matrix\n",
        "#input_shape = size of image matrix on which convolution is to be performed , train data=64 by 64 , 3 = RGB Colored images , 1 = b&w image\n",
        "#Relu activation function applied to break linearity in the image, also used in hidden layers of ANN\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape=[64, 64, 3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ9TQXs41iyo"
      },
      "outputs": [],
      "source": [
        "#Now applying & adding pooling layer on our convolved layer\n",
        "#When applying Maxpooling on our feature map/ convolve map , pool matrix of size 2 by 2 is applied & maximum of each square matrix\n",
        "#of ConvolvedMap is extracted & pool feature map is created - see slides\n",
        "#Pool size of 2 by 2 matrix is used to apply Maxpooling\n",
        "\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2 ))     #MaxPooling applied on Convolved layer\n",
        "\n",
        "#Another parameter Stride = no of pixels shift when pooling each 2 by 2 square matrix of convolved map = 2 pixels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewml5Ihu4jWG"
      },
      "outputs": [],
      "source": [
        "#Similarly 2nd hidden layer added & convolution & pooling performed in these layers too.\n",
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_EZO7XL7ZcO"
      },
      "outputs": [],
      "source": [
        "#Converting the pooled feature map to vector of one column\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "#Flatten class will take all pooled matrix & apply flattening to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvXsydlZ93jN"
      },
      "outputs": [],
      "source": [
        "#Now the flattened layer matrix containing vector of features is to be fed to ANN as input layer\n",
        "#Now first layer of Ann is added/implemented = i/p layer will be fully connected to Flattened layer\n",
        "#Here we're dealing with Computer Vision more complex problem than data mining\n",
        "#128 No of neurons hidden layers are initialised so as to obtain better accuracy\n",
        "\n",
        "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu', ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOL7E816AEHQ"
      },
      "outputs": [],
      "source": [
        "#Softamx as o/p layer activation function is used to range the probabilities of o/p b/w 0 & 1\n",
        "\n",
        "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "#Here convolutional neural network is build , pretty smart model with eyes to predict the image as Dog/Cat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8kITooyBsvL"
      },
      "outputs": [],
      "source": [
        "#Now compliling CNN with parameters to follow while classifying within hidden layers\n",
        "\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PjQ8ijR40d_",
        "outputId": "2439ede7-3ca4-47a8-bcdc-027a629e6a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - accuracy: 0.5528 - loss: 0.6862 - val_accuracy: 0.6690 - val_loss: 0.6097\n",
            "Epoch 2/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.6709 - loss: 0.6119 - val_accuracy: 0.7085 - val_loss: 0.5676\n",
            "Epoch 3/25\n",
            "\u001b[1m 14/250\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 99ms/step - accuracy: 0.6785 - loss: 0.5824 "
          ]
        }
      ],
      "source": [
        " cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
        "                         #fixed attribute name for test_set is validation_data\n",
        "\n",
        "#As no of epochs , Increasing the number of epochs means that the model will be trained for longer as dataset used is very large.\n",
        "#This can lead to better performance on the training set, but it also increases the risk of overfitting.\n",
        "#Overfitting occurs when the model learns the training data too well and does not generalize well to new data.\n",
        "\n",
        "#To mitigate overfitting, you can use techniques like early stopping, regularization, or data augmentation.\n",
        "\n",
        "#Each batch has 32 images , total 8000 images = 8000/32 = 250 batches would be trained for 25 times/epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_X7LMSb5rPA"
      },
      "outputs": [],
      "source": [
        "import numpy as np    #numpy required to deal with array created from image for prediction\n",
        "\n",
        "#Predict method only allows the same i/p as training set for prediction, so we have to perform these steps\n",
        "\n",
        "from keras.preprocessing import image  #To load single image from directory, submodule 'image' of preprocessing module from keras library is used\n",
        "test_image = image.load_img('dataset/single_prediction/a.jpg', target_size = (64,64)) #To import image from directory , class 'load_img'of image submodule is used, Also trained images were 64 by 64, so test_img should be of same dimension\n",
        "test_image = image.img_to_array(test_image) #As input of predict method is 2D array, so we need to convert the image to array using 'img_to_array' class of image sub module\n",
        "test_image = np.expand_dims(test_image, axis = 0) #Expand the shape of an array, As batches of images were given for training, another dimension for batches made in training phase, so we need to add that dimension as predict method take exact same i/p as train set, (np.expand_dims(array,axis))\n",
        "\n",
        "#Now predict method can be called\n",
        "result = cnn.predict(test_image)\n",
        "\n",
        "#Now obtained o/p is numerical = 0/1=cat/dog, so to make it categorical , we need to obtain for respective class\n",
        "training_set.class_indices  #we've got the right indices value representing each class at o/p layer for both classes\n",
        "\n",
        "if result[0][0] == 1:      #Then obtained indices compared to o/p and categorical value obtained\n",
        "  prediction = 'Dog'\n",
        "else:\n",
        "  prediction = 'Cat'\n",
        "\n",
        "  #result[0][0] - accessing the 0th batch & 0th element of that batch of result conatining final prediction.\n",
        "\n",
        "print(prediction)\n",
        "\n",
        "#Now implement this code on Jupyter notebook - is a platform used for deployment of ML codes having several python IDES like jupyter notebook\n",
        "#and spyder, provides environment for ml models and projects to deploy, have librarys to work with ML & DL Models\n",
        "#Install Anaconda Software on your Mac\n",
        "#Install Librarys as Tensorflow and keras on your system using command terminal.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}