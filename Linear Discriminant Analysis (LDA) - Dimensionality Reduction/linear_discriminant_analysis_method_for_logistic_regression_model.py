# -*- coding: utf-8 -*-
"""Linear_Discriminant_Analysis_Method_for_Logistic_Regression_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYdildk7QSN4DN8FP_jJMROdZVEKAgd1

# Linear Discriminant Analysis (LDA)
"""

# #Supervision: PCA is unsupervised, while LDA is supervised.
# Purpose: PCA focuses on capturing variance and reducing dimensionality, while LDA focuses on enhancing class separability.
# Output: PCA produces principal components, whereas LDA produces linear discriminants.
# Use of Labels: PCA does not use class labels, whereas LDA requires class labels for optimization.
# Application: PCA is used for data preprocessing and visualization, while LDA is used for feature extraction and classification.

#Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that optimally finds linear combinations of
#features to maximize the separation between multiple classes in the data.

#Dimensionality reduction in Independent variables + linear class separation in dependent variable,
#both X_train,y_train & X_test too are given as parameters for LDA implementation

#Supervised linear Class separation technique within the data

#Check difference for LDA VS Linear regression in slides
#In linear regression, based on features/Independent variable we've to classify the class for various rowdata by establishing correlation b/w independent & dependent variable
#Whereas in LDA, we've to create class separabilty that is diff classes in dependent variable.

#____________________________________________

#Real life dataset
#DATASET: Wine shop owner gave 10 wine features for different wines that customer bought, asks most talented DataScientist to provide
#clustering for various customers group for further prediction , using LDA.

#After you have predictive model, Owner if gets any new wine in his shop, he can use this model to predict which customers will purchase
#the new set of wines & predicts & advertise them for profit.

#______________________________________________

#Just implementing of LDA on train,test set is different , else same code as PCA (Principal component analysis)

#LDA & PCA both are part of preprocessing data techniques

#You can try the Prediction of model on various datasets too

"""## Importing the libraries"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Wine.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Applying LDA"""

#discriminant_analysis module & LinearDiscriminantAnalysis class for LDA Implementation

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2) #No of components/feature to extract from set of independent vars using correlation b/w them = 2
X_train = lda.fit_transform(X_train, y_train)   #Dimensionality reduction in Independent vars + linear class separation in dependent variable
X_test = lda.transform(X_test)

#After this execution, X_train will've two Component classes that are highly correlated out of all = LD1 & 2 (X & Y axis),
#And X_test will have linearly separate classes within

"""## Training the Logistic Regression model on the Training set"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Congrats , 100% accuracy obtained on logistic regression implementation using Dimension reduction & class separation using LDA.
#That's very rare to achieve situation as data-scientists

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.legend()
plt.show()


#So as per Graphs obtained, LDA separates class in a way that they totally distinguish each other with great margins, and representation
#orientation on graph is done in a way that each classes classify without errors

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.legend()
plt.show()