# -*- coding: utf-8 -*-
"""Principal_Component_Analysis_On_Logistic_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16HIxtlTTOg6obaYqoFRiBvVXTXfj_aUX

# Principal Component Analysis (PCA)
"""

# #Illustration:
# Remember in Part 3 - Classification, we worked with datasets composed of only two independent variables. We did for two reasons:

# Because we needed two dimensions to visualize better how Machine Learning models worked (by plotting the prediction regions and
# the prediction boundary for each model).

# Because whatever is the original number of our independent variables, we can often end up with two independent variables by applying
# an appropriate Dimensionality Reduction technique.

# There are two types of Dimensionality Reduction techniques:
# Feature Selection
# Feature Extraction

# Feature Selection techniques are Backward Elimination, Forward Selection, Bidirectional Elimination, Score Comparison and more.
# We covered these techniques in Part 2 - Regression.

# In this part we will cover the following Feature Extraction techniques:
# Principal Component Analysis (PCA)
# Linear Discriminant Analysis (LDA)
# Kernel PCA
# Independent Component Analysis (ICA)
#_______________________________________________

#PCA is UnSupervised TechniwueNo Supervision of Outcome: PCA does not rely on labeled outcome variables (like class labels in
#classification or target variables in regression). It operates solely on the input data matrix to find the principal components.

#supervised learning techniques, such as classification and regression algorithms, require labeled data to train models that predict
#outcomes based on input features. PCA, however, is used as a preprocessing step in many supervised learning tasks to reduce the
#dimensionality of the feature space, potentially improving the performance and efficiency of subsequent supervised models.

#________________________________________________

#Dimensionality Reduction : Not part of ML , But used to handle the big data with large dimensions/features
#To work with such data, we need to reduce the complexities = dimensions
#PCA is one of the imp Unsupervised learning algo used for the dimensionality reduction
#Find the variations within dataset & detect the correlation b/w the variables, while retaining the most of the information.
#IMP : Goal of PCA is to reduce the dimensions of d-dimensional-dataset by projecting it on k-dimensional-subspace (K<D).
#Unlike linear regreesion PCA is not predicting the values , but PCA is learning relationships b/w the variables (X & Y axis) -
#See Visualisation : https://setosa.io/ev/principal-component-analysis/
#PCA is Highly affected by outliers in data.

#Dataset wine.csv has more than 10 indepemdent variables, each row depicts diff wines, colums have features of each wine depicted,
#Required to classify the different customers into clusters based on diff wines = 3 clusters for similar wine preferences of customers.
#Recommender system to predict if new wine comes, So recommend to which segment of new customers will be most appropriate & will add value to owner, optimize sales & profit.
#In PCA, We're not reducing the features, but we'll find the two principal component for the prediction out of all features.
#You've to apply PCA = dimensionality reduction on training set before applying algortihm for dataset training
#Applying on both Train & test data , as predict method takes exact format of i/p as it was trained on.

#So PCA does is from multiple featured data/ multiple independent variable, It will find relation and correlation b/w various features, then
#it will provide the new dataset with only two features for prediction.  From 10 to 2 features (2d) - dimensionality reduced.
#We may increase the features to 3 or more, if we are unable to predict/classify greatly the no of categorys in our dataset using those linear boundaries properly.

"""## Importing the libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## Importing the dataset"""

dataset = pd.read_csv('Wine.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:,-1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train , X_test, y_train, y_test = train_test_split(X,y , test_size = 0.20 , random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Applying PCA"""

from sklearn.decomposition import PCA   #Decompostition module's PCA class from scikitlearn Library id used to import Pca package
pca = PCA(n_components = 2)    #No of feature/Principle component to extract out of all features after dimensionality reduction on dataset
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# pca.fit_transform(X_train): This operation fits the PCA model on the training data to learn the principal components and then
# transforms the training data into the new reduced feature space.

# pca.transform(X_test): This operation transforms the test data into the new reduced feature space using the principal components
# learned from the training data.


#What the fit will do is access all the features from train set & provide to pca to obtain the two Pca component & then transform
#those valuesto be saved into new matrix

#Only apply fit on train , not on test as it will lead to information leakage

"""## Training the Logistic Regression model on the Training set"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test,y_pred)

#IMP :::: Woohoo, achieved very great ACCURACY of 97% , PCA dimensionality reduction not only reduces the complexity of dataset,
#but also boosts up the accuracy of model for such large dataset with many features (10)
#By applying (Dimensionality Reduction) + (Predictive model) give very high accuracy

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()


# Then plotting obtained two features , Principle component 1 & 2 using PCA on x & y axis

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

#Only one of the value is predicted in wrong class