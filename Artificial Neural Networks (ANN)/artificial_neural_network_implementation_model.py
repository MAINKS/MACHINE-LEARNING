# -*- coding: utf-8 -*-
"""Artificial_Neural_Network_Implementation_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MwgNeAERriFHiZr2h0O7P8mHMoQJqDFI

# Artificial Neural Network

### Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf   #tensorflow is preinstalled library in google collab - it's used for deep learning neural networks implementation.
tf.__version__   #to check the installed version of tensorflow , 2.17.0 is latest

"""## Part 1 - Data Preprocessing
Data preprocessing : - It counts for 70% of work for a data scientists

### Importing the dataset
"""

dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:,3:-1].values #our dataset(view),first 3 columns = rowid,CustomerID & Surname , have no correlation with dependent variable , So only required relative columns chosen.
y = dataset.iloc[:,-1].values #Dependent variable having only last column.

print(X)
print('\n')
print(y)

"""### Encoding categorical data

Label Encoding the "Gender" column
"""

#So in Matrix X of independent variables, we've two columns with categorical / alphabetical data - Gender(2nd column) and
#geography (1st column) , Gender can be labelencoded as it've two categorys only
#Geography have more than 2 categorical instances- onehotencoding is used (100 ,010,001)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X[:,2] = le.fit_transform(X[:,2])
print(X)

"""One Hot Encoding the "Geography" column"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder                  #column to onehotencode
ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1] )], remainder = 'passthrough')
X = np.array(ct.fit_transform(X))
print(X)

"""### Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train , X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2 , random_state = 0)

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)
print(X_train)
print('\n')
print(X_test)

"""## Part 2 - Building the ANN

### Initializing the ANN
"""

#API - Application Programming Interface
#Building the sequence of layers of Neural networks from the i/p to the o/p layer & hidden layer in between
#Using the sequential function to create in sequence ,
#Computational graphs as non sequential  is boltzmann machine, neurons connected in any way not in successive layers. (not covered here)
#So to implement ANN Sequential neuron layers , Tensorflow2.0 library/Api to ðŸ‘‰ Keras Api ðŸ‘‰ model module ðŸ‘‰ sequential class/attribute is imported
#First keras and tensorflow were separate , now they integrated together in Tensorflow2.0 update
#Keras was separate library/API before , now keras is module within tensorflow2.0
#Sequential class is used to implement ANN
#Error : module 'keras.api._v2.keras.models' has no attribute 'sequential'

ann = tf.keras.models.Sequential()     #In sequence - i/p - hidden layer - o/p layer  , classes always have first letter capital

#no of i/p neurons in i/p layer are automatically detected by tensorflow & created acc to no of independent variables = i/p neurons

#ann is variable as an object of this imported Sequential class

#Now above we've created a neural network in sequential order , skeletal structure of ANN

"""### Adding the input layer and the first hidden layer"""

#Add function will help add a layer
#Famous "dense" class in tensorflow and pytorch libraries is used to add fully connected layer into an ANN at any stage of ANN
#path to call this class is (tf library)-(keras)-(layers module) - (Dense class)
#As within layers we've no of neurons to be added within each layer
#Parameters within this class is Units- no of neurons - most asked ques in deep learning - how to determine the no of neurons in each layers
#It's done on experimentn basis - used six neurons majorly
#calling object of sequential class ann to add an i/p layer to it

ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))     # classes always have first letter capital
                             #No of neurons

#2nd parameter is Activation fxn to be assigned within neurons = Rectifier activation fxn, representd by 'ReLU'
#Activation fxn breaks the linearity b/w the i/p and hidden layer & execute the i/p data to provide the o/p
#In this way we've created the first layer of our Neural Network


#No activation function required for the regression as o/p is numerical,
#only for classification, sigmoid (2 categories) or softmax(3 categories) activation fn it is required as we Obtain categorical data of larger scope

"""### Adding the second hidden layer"""

#Adding 2nd layer is same as adding initial i/p layer
ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))   #Dense class creates the linkage b/w i/p Neurons and hidden layer neurons of ANN.

#Add method can add layer in ANN at any stage of the imlementation

"""### Adding the output layer"""

#Adding o/p layer will take diff parameters else same
ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

#Using the rectifier activation function in hidden layer & Sigmoid activation function in the o/p layer - probabilistic way to get binary
#outcomes , whether user will leave or not in probabilistic value.

#when o/p is binary , activation function = sigmoid
#when o/p is non-binary , activation function = softmax (-âˆž to +âˆž on x axis, yaxis = 0 to 1 , threshold at 0.5 )

#Here the neural network is implemented - (Artificial neural brain)

"""## Part 3 - Training the ANN

### Compiling the ANN
"""

#After ANN is designed , it's to be trained now
#Compliling the ANN with optimizer and loss function and a metric = accuracy as we're dealing with classification here.

# Optimizer , loss function & metrics = accuracy are parameters of 'compile' function in tensorflow

ann.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])  #metrics as accuracy is used

#Optimizer is the tool by which your'e going to perform the stochastic gradient descent
#Optimizer is connected to take care of loss during back-propagtion for weight updation

#Optimizer = which method to evaluate with = stochastic gradient descent = 'adam' , weight updation is done via stochastic method
#loss function used in binary outcome is used = binary_crossentropy
#The way ANN Will evaluate the Cost function is given as loss ='' as parameter
#loss function used in Non-binary outcome is used = categorical_crossentropy
#mean_squared_error is loss fxn while implementing ANN in Regression , binary_crossentropy for classification

"""### Training the ANN on the Training set"""

#Now training of ANN by feeding the training data is to be done:
#fit function is used to train the ANN , it will take the parameter from dataset and train ANN on it.

ann.fit(X_train , y_train, batch_size = 32 , epochs = 100)

#For Batch learning, batch_size is used as we're going to have the batch weight updation , default is 32 , you can tune the hyperparameter too.
#Meaning out of 1000 rows , 32 in each batch are splitted are fed.
#epochs is the cycles the training data is to fed to th ANN network to improve accuracy ,1 cycle = 1 epoch
#For each epochs , error is minimised and accuracy is increased.

#We've made the skeletal ann structure ,added i/p , hidden and o/p layer , then provided method/activË†n fxn for hidden neurons to work ,
# for back proagation , which method/optimizer, loss fxn to use and at last fitting the dataset and trained our ANN On given data.
#(Working) - For these value of independent variable , we have these o/p and would be run 1000 times & model will crate the relation b/w the variables & further predict the value for test data.
#Column being fixed at neurons , each row data one by one is feed into neuron till o/p & ANN builds a correlation.
#In this way complete ANN is implemented & further is to be predicted

"""## Part 4 - Making the predictions and evaluating the model

### Predicting the result of a single observation



Use our ANN model to predict if the customer with the following informations will leave the bank:

Geography: France

Credit Score: 600

Gender: Male

Age: 40 years old

Tenure: 3 years

Balance: \$ 60000

Number of Products: 2

Does this customer have a credit card? Yes

Is this customer an Active Member: Yes

Estimated Salary: \$ 50000

So, should we say goodbye to that customer?

**Solution**
"""

#1st - As we've done the encoding on our categorical data , we need to check , France = 1,0,0 & male = 1
#2nd - Also we've trained our data on feature scaled values, we need to provide scaled values only to the model for further predicting on test data too.

print(ann.predict(sc.transform([[1, 0, 1, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))) #yields probability = 0.04 prob to leave bank
print(ann.predict(sc.transform([[1, 0, 1, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) >0.5) #yields categorical value
#here we set the threshold to 0.5 , if above it true will be printed & below false will be printed


# we can also do this
if ann.predict(sc.transform([[1, 0, 1, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) >0.5:
  print("Customer will leave the bank")
else:
  print("Don't worry customer won't leave the bank")

#we implemented all 3 ways here to print the o/p in different ways - prob - categorical - if else.

#So transform the values first to same scale feature scaling is done , then predict those values using ANN
#Also we've used the Sigmoid fxn on o/p layer gives probability , we need to change the probability to categorical value as :

"""Therefore, our ANN model predicts that this customer stays in the bank!

**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the "predict" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.

**Important note 2:** Notice also that the "France" country was not input as a string in the last column but as "1, 0, 0" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, "France" was encoded as "1, 0, 0". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns.

### Predicting the Test set results
"""

y_pred = ann.predict(X_test) > 0.5   #again o/p = prob , convert to categorical by adding threshold on sigmoid obtained values =0.5
print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))

#numpy as np is used to deal with numerical calcualtion , matrix creation and altering modifying values in implementation.
#concatenate - create a matrix with given i/p parameters
#In this way , we printed both predicted and actual values in a matrix side by side

#we've got actual values to model predicted value matrix below
# 1 = leaves 0 - stays in bank

"""### Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_pred, y_test)
print(cm)
accuracy_score(y_pred, y_test)

#out of 100 - 86 customers predicted correctly