# -*- coding: utf-8 -*-
"""K_Fold_Cross_Validation_technique_on_KernelSVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LDrrvwD_cAM2tQomPRmRD_zWBu4cqGwI

# k-Fold Cross Validation
"""

#K -Fold cross validation - Model selection Technique

#Evaluating the model performance using K fold Cross Validation
#Variance - when accuracy of model is high - it predicts the accurate results after training on train set ,
#but when a new set is fed for prediction, it shows high variance b/w actual to predicted value that is high variance condition.

#So testing the model only on our test set is not enough for actual accuracy as variance can be seen on new dataset,
#To avoid this variance problem, we'll be using K-Fold cross validation technique on our train set while training model.
#Train on K-1 folds and test on validation data, then upto kth fold it is done, for each fold, accuracy is measured and then average &
#standard deviation of the all 10 folds acuracy is calculated to obtain the variance in them.
#Bias(error in predictions) is opposite of accuracy.

#After our model is trained on training set, K-Fold is applied afterwards of it.
#It will return the 10 accuracies for all 10 different folds executed.

"""## Importing the libraries"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Training the Kernel SVM model on the Training set"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""## Applying k-Fold Cross Validation"""

#model_selection module have cross_val_score class for the KFoldCrossValidation implementation

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train , y = y_train, cv = 10)
print("Accuracies: ", accuracies)
print("Average Accuracy: ", accuracies.mean())
print("Standard Deviation: ", accuracies.std())


#Classifier is the object of data fitted on which model is trained upon in the training phase
#Then provide Training data X_train, y_train for K-Fold cross validation
#cv parameters is the no of folds that K-Fold validation will apply on Train data (10 Used majorly)
#So Evaluating the model on our 10 different sets will provide much better accuracy & help minimize variance.

#After accuracies are obtained, average & standard deviation needs to be calculated
#Standard Deviation is standard difference in the values obtained
#90% Accuracy & 6% variance is obtained - low bias & low variance

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)
plt.title('Kernel SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)
plt.title('Kernel SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()