{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAINKS/MACHINE-LEARNING/blob/main/data_1preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uslDsLMtYtu2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "outputs": [],
      "source": [
        "#Librarys are the symobol of modules containing functions and classes used to perform operations and actions. ex scikit learn contains all Ml required classes that we can use to create object of.\n",
        "\n",
        "#Numpy - will allow us to work with arrays as Ml model requires array as input.\n",
        "\n",
        "#Matplotlib - to plot graphs and charts , .pyplot is a module\n",
        "\n",
        "#Pandas - allow us to import datasets & create matrix of features & dependent vector variable.\n",
        "\n",
        "#Data slicing is done [1:9,8:10]\n",
        "#Scikit learn is the library highly used in ml & have a module \"model selection\"used for Data preprocessing.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#!pip install scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[: , :-1].values\n",
        "y = dataset.iloc[: , -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCsz2yCebe1R",
        "outputId": "fa7bf1f1-e54c-45bc-ea3c-72cb144d6616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYrOQ43XcJR3",
        "outputId": "f65c98aa-b48e-42ec-b94c-2329955ac13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KT9WiZ346A"
      },
      "source": [
        "# Take care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD1R6KFj4MG8",
        "outputId": "283e3d23-e021-4de5-8cb7-e6c7ffdc57bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "#Using average of all salarys , you can get the missing values in the columns.\n",
        "\n",
        "#Scikit learn is library used to do this task. Class \"simpleimputer\" will be used.\n",
        "\n",
        "#Imputer is created object of class \"simpleimputer\"\n",
        "\n",
        "#Providing only numeric values(1st & 2nd col) to imputer using fit keyword.\n",
        "\n",
        "#All rows and only columns with numeric data from X Matrix containing independent variables.\n",
        "#Transform is used to update the values in the matrix.\n",
        "\n",
        "from sklearn.impute import SimpleImputer  #importing simpleimputer from scikit library\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  # creating object of class simpleimputer indicating task to perform & method\n",
        "imputer.fit(X[:,1:3])   # accessing missing values from matrix\n",
        "X[:,1:3]=imputer.transform(X[:,1:3])  # Updating missing values\n",
        "print(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hwuVddlSwVi",
        "outputId": "6e7c20c4-a3f0-41d7-f140-ef5cbb536ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "# Converting categorical data into numerical data ,Country column containing (germany,france,spain) will be converted to numerical data.\n",
        "# Germany france spain will be categorized individually\n",
        "# Dependent variable (purchase) will be (Yes-1 , No - 0).\n",
        "# One columntransfer class of compose module from scikit-learn library is used.\n",
        "# Second class Onehotencoding from preprocessing module of scikit library is used to achieve this conversion.\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "#      library  module        class name\n",
        "\n",
        "# a new object (ct) of columntransformer class is created\n",
        "#Matrix x is then transformed and fitted , stored in object ct as an array.\n",
        "\n",
        "# MOST IMPORTANT:\n",
        "#ColumnTransformer with Onehotencoder will be applied when column has variables as (germany,france,spain)\n",
        "#LabelEncoder will be applied when column has variables as (Yes,No)\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct=ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[0])] , remainder='passthrough')\n",
        "#Object   class       details        encoder    encoding class  columns to transform.    remainder req to mention\n",
        "X = np.array(ct.fit_transform(X))    #O/p saved in array , so array conversion is required\n",
        "print(X)\n",
        "\n",
        "\n",
        "#O/p - 10100 france 01001 spain 01010 germany are encoded in in binary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ6QpmuPOj-7"
      },
      "source": [
        "#Encoding Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgHCShVyTOYY",
        "outputId": "2656e94e-b1a0-409a-b2f1-6a3300922803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "#Importing class labelencoder from preprocessing module of sklearn library\n",
        "#create an object name le for class label encoder\n",
        "#applying fit & transform on Matrix y using object le created from labelencoder class to encode it to numerical value\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "y=le.transform(y)\n",
        "       #or\n",
        "y=LabelEncoder().fit_transform(y) #for ease\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXgA6CzlqbCl",
        "outputId": "ee7a1308-04cc-44f2-98b6-69107bd8711f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n",
            "\n",
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n",
            "\n",
            "[0 1 0 0 1 1 0 1]\n",
            "\n",
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "#IMP #Feature scaling is always applied after splitting data into training & test dataset as if\n",
        "#done before , variation in the dataset will be observed due to mean standardisation that is\n",
        "#not required in the test dataset used for model training - To prevent information leakage - as test set is supposed to be something new.\n",
        "\n",
        "#Module \"Model selection\" with a defined class \"Train_test_split\" is used for spliiting dataset.\n",
        "#4 matrices two for train & two for test dataset with each of dependent & independent variable.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=1)\n",
        "\n",
        "# X & y matrices of train/test data  class name  Input as complete matrix X & y  data split size = 80/20  randomly splitting\n",
        "\n",
        "print(X_train)\n",
        "print(\"\")\n",
        "print(X_test)\n",
        "print(\"\")\n",
        "print(y_train)\n",
        "print(\"\")\n",
        "print(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxjSUXFQqo-3",
        "outputId": "a98e049a-c41c-4336-8bd8-9214764445bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
            "\n",
            "\n",
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ],
      "source": [
        "# Feature scaling allows us to put all our data features on the same scale.\n",
        "#To avoid some features dominating other features. To have all th value of features in the same range.\n",
        "#Standardisation & normalisation\n",
        "#Normalisation is preferred when we have normal features in most of the distrubiution\n",
        "#Standardisation works in majorly all the conditions and is more preferred.   X_stand = X - mean(x)/StandardDeviation(X)\n",
        "#Feature scaling is not done on test set , performed over train set & then transformed onto test set.\n",
        "\n",
        "#StandardScaler class for standardisation from preprocessing module of scikitlearn library is imported.\n",
        "#StandardScaler class will automatically calculate mean and steandard deviation of X matrix & standardise the features of matrix and scaling is done.\n",
        "\n",
        "\n",
        "#Most Asked ques by DataScience Community : Do we have to apply feature scaling on dummy variables in a matrix of features\n",
        "#Answer : No , as dummy variables are already in the same scale range as other variables.\n",
        "#Only apply Standardisation feature scaling to your numerical values only - Age & salary , don't apply on Country as it's dummy variable. Slicing of columns will be done.\n",
        "#On X_train & test matrix it is applied , not on y matrix containing categorical data.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()   #object created\n",
        "X_train[: , 3:]=sc.fit_transform(X_train[: , 3:])\n",
        "X_test[:,3:]=sc.transform(X_test[:,3:])  #same scale used on both test and train matrix of X(Numerical data)\n",
        "#All Rows & 3rd col onwards.    fit will calc the mean & StDeviation & transform will put into formula & calculate.\n",
        "print(X_train)\n",
        "print(\"\\n\")\n",
        "print(X_test)\n",
        "\n",
        "\n",
        "#Scaling of features age & salary done b/w range -2 & 2.\n",
        "#Will help optimise training of ML model.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}